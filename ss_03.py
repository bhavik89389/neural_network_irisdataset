# -*- coding: utf-8 -*-
"""SS_03.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1flZgX-l9Cu9Z_5ZBhX5xRKzqhGtttj2w
"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder

# load the dataset

iris = load_iris()

X = iris.data  # if there are 150 rows and there are 4 features

X.shape

y = iris.target.reshape(-1,1)

y.shape

np.unique(y)

encoder = OneHotEncoder(sparse_output = False)
y_encoded = encoder.fit_transform(y)

y_encoded

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

X

X_scaled

X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_encoded, test_size=0.2, random_state=42)

b=X.shape

b[0]

b[1]

X.shape[1]

input_size = X.shape[1]  # 4
hidden_size = 8
output_size = 3
lr = 0.1
epochs = 3000

def get_activation_funcs(name):
  if name == "sigmoid":
    act = lambda x : 1/(1 + np.exp(-x))
    deriv = lambda x : x * (1-x)
  elif name == "tanh":
    act = np.tanh
    deriv = lambda x : 1 - (x**2)
  elif name == "relu":
    act = lambda x : np.maximum(0,x)
    deriv = lambda x : np.where(x>0,1,0)
  elif name == "leaky_relu":
    act = lambda x : np.maximum(0.01*x,x)
    deriv = lambda x : np.where(x>0,1,0.01)
  else:
    raise ValueError("Invalid activation function name")
  return act, deriv

def softmax(x):
  exp_x = np.exp(x  - np.max(x, axis= 1, keepdims= True))
  return exp_x / np.sum(exp_x, axis= 1, keepdims= True)

def cross_entropy(y_true, y_pred):
  return -np.mean(np.sum(y_true * np.log(y_pred + 1e-8), axis =1))

def accuracy_score(y_true, y_pred):
  return np.mean(np.argmax(y_true, axis = 1) == np.argmax(y_pred, axis = 1))

def train_model(activation_name):
  activation, activation_deriviative = get_activation_funcs(activation_name)
  loss_history = []
  accuracy_history = []

  np.random.seed(1)
  W1 = np.random.randn(input_size, hidden_size)
  b1 = np.zeros((1, hidden_size))
  W2 = np.random.randn(hidden_size, output_size)
  b2 = np.zeros((1, output_size))

  # training starts here

  for epoch in range (epochs):

    # forward propagation or forward pass
    Z1 = np.dot(X_train, W1) + b1  # Z1 = w1.x1 + b1
    A1 = activation(Z1)           # A1 = relu (Z1)
    Z2 = np.dot(A1, W2) + b2      # Z2 = A1.W2 +b2
    A2 = softmax(Z2)           # A2 = softmax(Z2)

    loss = cross_entropy (y_train, A2)
    loss_history.append(loss)

    # backward propagation or backward pass
    dZ2 = A2 - y_train
    dW2 = np.dot(A1.T, dZ2)
    db2 = np.sum(dZ2, axis = 0, keepdims = True)

    dA1 = np.dot(dZ2, W2.T)
    dZ1 = dA1 * activation_deriviative(A1)
    dW1 = np.dot(X_train.T, dZ1)
    db1 = np.sum(dZ1, axis = 0, keepdims = True)

    # update weights
    W2 = W2 - lr*dW2
    W1 = W1 - lr*dW1
    b2 = b2 - lr*db2
    b1 = b1 - lr*db1

  # Define the predict function within train_model as before
  def predict(X):
    A1 = activation(np.dot(X, W1) + b1)
    A2 = softmax(np.dot(A1, W2) + b2)
    return A2

  y_pred = predict(X_test)

  # print ("Test", y_test.shape, y_pred.shape)
  acc = accuracy_score(y_test, y_pred)

  # Return the loss history, accuracy, and the predict function
  return (loss_history, acc, predict)

# Define the list of activation functions to use
activations = ["sigmoid", "tanh", "relu", "leaky_relu"]
accuracies = {} # Initialize accuracies dictionary

for act in activations:
  # Unpack the three values returned by train_model
  losses, acc, _ = train_model(act)
  accuracies[act] = acc
  plt.plot(losses, label = f'{act} (acc ={acc: .2f})')
  print("losses", len(losses))

plt.title ("Loss curve for MLP 3 class : Iris dataset")
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.legend()
plt.show()

# ex--1
_, _, predict_fn = train_model("relu")

# Normalize the sample
sample = np.array([[5.1, 3.5, 1.4, 0.2]])
sample_scaled = scaler.transform(sample)

# Predict
predicted_probabilities = predict_fn(sample_scaled)[0]
# Find the class index with the highest probability
predicted_class_index = np.argmax(predicted_probabilities)
print("Predicted class index:", predicted_class_index)

# ex--2
_, _, predict_fn = train_model("relu")

# Normalize the sample
sample = np.array([[1.8, 6.2, 0.4, 0.8]])
sample_scaled = scaler.transform(sample)

# Predict
predicted_probabilities = predict_fn(sample_scaled)[0]
# Find the class index with the highest probability
predicted_class_index = np.argmax(predicted_probabilities)
print("Predicted class index:", predicted_class_index)